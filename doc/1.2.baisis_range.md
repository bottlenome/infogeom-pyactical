# 距離の概念（フィッシャー情報量による計量）
## 確率分布の距離とは？

「確率分布の間の距離」とは何でしょうか？ふつう私たちは、町と町の間の距離をメートルなどで測ります。同じように、確率分布（起こりうる結果とその確率のまとまり）の間にも、
「違いの大きさ」を距離として測れると考えます。
二つの確率分布がよく似ていれば距離は小さく、まるで2つの点が近くにあるようなイメージです。
反対に、大きく異なる二つの分布は遠く離れている（距離が大きい）と考えることができます。
もちろん、ここでいう「距離」は実際の長さではなく、分布の違いを表す指標です。 
例えば、コインの出る目の分布を比べてみましょう。以下の3つのコインA, B, Cを考えます。
コインA: 表が50%、裏が50%の確率（公平なコイン）
コインB: 表が60%、裏が40%の確率（少し偏ったコイン）
コインC: 表が90%、裏が10%の確率（大きく偏ったコイン）
この場合、公平なコインAと少し偏ったコインBの分布は比較的近い（距離が小さい）と言えます。一方、公平なコインAと大きく偏ったコインCの分布はかなり異なり（距離が大きい）と言えます。さらに、もしまったく同じ分布同士を比べれば、その距離は0になります。要するに、確率分布の距離とは「どれくらい分布（確率の割り振り方）が違っているか」を表すものなのです。

## フィッシャー情報計量の基本
情報幾何では、この距離を測るために「フィッシャー情報計量」という考え方を用います。
フィッシャー情報計量は、確率分布がパラメータの変化に対してどれくらい敏感に変化するか（どれだけ情報を持っているか）を基に距離を定めます。
直感的には、パラメータを少し変えると分布がどれだけ変わるかを測り、その変化のしやすさを距離の尺度とするイメージです。
パラメータの変化によって分布が大きく変わるなら距離も大きく、ほとんど変わらないなら距離は小さくなります。 
フィッシャー情報計量を表す中心的な指標がフィッシャー情報量です。フィッシャー情報量は統計学の用語ですが、簡単に言えば「その分布に含まれる情報の量（パラメータに関する感度）」を表す値です。
実はフィッシャー情報量 $I(\theta)$ は次のような数式で定義されます（$\theta$は分布のパラメータです）。

$$I(\theta)=\mathbb{E}[(\frac{d}{d\theta}\ln p(X;\theta))^2]$$

難しそうに見えますが、各部分を噛み砕いてみましょう。
この式では$\ln p(X;\theta)$は確率の対数（ログ）です。
そして$\frac{d}{d\theta}$はパラメータ$\theta$の微小な変化に対する変化率（微分）を表しています。
つまり、「パラメータをほんの少し変えたときに、各結果の起こる確率（の対数）がどれだけ変わるか」を示す指標です。
その変化の大きさを二乗して平均（期待値 $\mathbb{E}[,\cdot,]$）を取ったものがフィッシャー情報量になります。
要するに、パラメータに対する分布の変化のしやすさを表す数値だと考えてください。 
フィッシャー情報量がわかると、確率分布間の距離を測ることができます。
パラメータが少し変化したときの「距離の二乗」は、この $I(\theta)$ に比例します（専門的には $ds^2 = I(\theta), (d\theta)^2$ の関係があります）。
平たく言えば、フィッシャー情報量が大きいほど同じパラメータ変化でも距離が大きくなり、フィッシャー情報量が小さいと距離は小さくなります。 

例: コイン投げの分布（ベルヌーイ分布）でフィッシャー情報量を見てみましょう。
パラメータ $p$（表が出る確率）に対するフィッシャー情報量は、実は次のように求められます。

$$I(p)=\frac{1}{p(1-p)}$$

この結果から、例えば $p=0.5$ では $I(0.5) = 4$、$p=0.9$ では $I(0.9) \approx 11.11$ となります。
$p$が極端な値（0や1に近い）にあるほど $I(p)$が大きくなることがわかります。
つまり、コインの例では$p$が0.5付近では分布は比較的安定ですが、$p$が0や1に近いときはわずかな変化で分布が大きく変わる（「絶対起きないはずの裏が少し起きるようになる」等）ため、
フィッシャー情報量が大きくなるのです。
このようにフィッシャー情報計量では、分布の性質に応じて距離の感じ方（尺度）が変化します。
## Python実装
では、実際に簡単な例でフィッシャー情報量を計算してみましょう。
以下のPythonコードは、コイン投げの確率分布（ベルヌーイ分布）についてフィッシャー情報量を計算するものです。
パラメータ$p$を入力すると、先ほどの式 $I(p) = \frac{1}{p(1-p)}$ に従ってフィッシャー情報量を求めます。

```python
# コインのフィッシャー情報量を計算する関数
def fisher_information_for_coin(p):
    # 各結果（0=裏, 1=表）の確率
    prob_tail = 1 - p  # 裏が出る確率
    prob_head = p      # 表が出る確率
    # 確率の対数の変化率（微分値）
    # 表が出た場合: d/dp ln(p) = 1/p
    # 裏が出た場合: d/dp ln(1-p) = -1/(1-p)
    deriv_tail = -1 / (1 - p)
    deriv_head =  1 / p
    # フィッシャー情報量の計算（期待値: 確率×微分値^2 の和）
    fisher_info = prob_tail * (deriv_tail ** 2) + prob_head * (deriv_head ** 2)
    return fisher_info

# 関数を使っていくつかのpでフィッシャー情報量を表示してみます
print(fisher_information_for_coin(0.5))  # p = 0.5 のとき
print(fisher_information_for_coin(0.9))  # p = 0.9 のとき
```
上記のコードを実行すると、p = 0.5 の場合は4.0、p = 0.9 の場合は約11.11という結果が得られます。
これは先ほど説明した通りの値になっています。
つまり、偏りが大きいコイン（pが極端な値）ではフィッシャー情報量が大きく、偏りが小さいコイン（pが中間の値）ではフィッシャー情報量が小さいことが確認できました。
フィッシャー情報計量を使うことで、このように確率分布間の距離を数値で表し、分布同士の違いを客観的に比較できるのです。
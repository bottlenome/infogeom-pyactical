# 1.1 情報幾何とは何か
情報幾何学（Information Geometry）とは、確率やデータの世界を地図（空間）のように考えて理解する数学の分野です​。
普通の幾何学が「形や図形」を扱うのに対して、情報幾何学は「確率分布」を扱います。
例えばサイコロの例を考えてみましょう。
すべての目が同じ確率で出る公平なサイコロと、
特定の目が出やすい不公平なサイコロでは、それぞれ確率の分布（出目のしやすさ）が異なりますよね​。
情報幾何ではそれぞれの確率分布を空間上の点と見立てます。
確率分布が違えば「空間」の中で位置が違うと考え、分布間の関係や変化を調べるのが情報幾何学なのです。

## 情報幾何の基本概念: 確率分布の「距離」を測る
確率分布の空間では、「分布同士の距離」に相当するものを定義できます。
情報幾何学で特に重要な概念には、フィッシャー情報計量とKLダイバージェンスがあります。
それぞれ確率分布の違いを測るための指標です。
### フィッシャー情報計量（Fisher Information Metric）
確率分布同士の違いを測るための尺度です​。
例えば、よく似た二つの天気予報（「晴れ90%」と「晴れ88%」など）でも、この計量を使えばどれくらい違うかを数値で表すことができます​。
直感的には、フィッシャー情報計量は確率分布間の距離を測る物差しのようなものだと考えることができます。
### KLダイバージェンス（Kullback-Leibler Divergence）
二つの確率分布の違いを表すもう一つの指標です​。
KLダイバージェンスが0であれば二つの分布はまったく同じ、数値が大きいほど分布が異なることを意味します​。
こちらは厳密には「距離」ではありませんが、分布間の差異を定量化するためによく使われます。

### Python実装例: 簡単な分布の可視化と距離計算
それでは、簡単な例で確率分布とその違いを見てみましょう。
以下にPythonコードを使ったデモンストレーションを示します。
サイコロを例に、
公平なサイコロ（各目が出る確率が全て同じ）と不公平なサイコロ（ある目だけ出やすい）の2つの分布を用意し、
それらの間のKLダイバージェンスを計算してみます。
```python
# 公平なサイコロの確率分布 (各目が1/6ずつ)
P_exact = [1/6]*6
# 不公平なサイコロの確率分布 (例えば6の目が他より出やすい)
Q_exact = [0.1]*5 + [0.5]

import math
# KLダイバージェンスを計算する関数
def kl_divergence(P, Q):
    return sum(p * math.log(p/q) for p,q in zip(P, Q))

# KLダイバージェンスを計算（Pを基準にQとの違いを測る）
kl_value = kl_divergence(P_exact, Q_exact)
print("P (公平なサイコロ):", [round(p, 3) for p in P_exact])
print("Q (不公平なサイコロ):", [round(q, 3) for q in Q_exact])
print("KLダイバージェンス(P||Q):", round(kl_value, 3))
```
実行すると次のような出力が得られます（Pは公平なサイコロ、Qは不公平なサイコロの分布です）:
```less
P (公平なサイコロ): [0.167, 0.167, 0.167, 0.167, 0.167, 0.167]  
Q (不公平なサイコロ): [0.1, 0.1, 0.1, 0.1, 0.1, 0.5]  
KLダイバージェンス(P||Q): 0.243  
```
この結果から、KLダイバージェンスが約0.243であることが分かります。
0ではない適度な値になっているのは、二つの分布が完全に同一ではなく違いがあることを示しています
（もしまったく同じ分布ならKLダイバージェンスは0になります）。
このように、情報幾何学では確率分布を空間上の点とみなし、数学的に「距離」や差異を定義して計算することができるのです。